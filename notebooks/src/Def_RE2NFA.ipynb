{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Regular Expressions, conversion to NFA\n",
    "\n",
    "In this module, we will cover regular expressions by showing how they can be converted to NFA. The scanner and parser for RE to convert them to NFA are the main part of this module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from jove.Def_NFA import mk_nfa\n",
    "from lex                 import lex\n",
    "from yacc                import yacc\n",
    "from jove.StateNameSanitizers import ResetStNum, NxtStateStr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Parsing regular expressions : ReParse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# reparseNEW.py\n",
    "#\n",
    "# Parses regular expressions (without the empty RE case)\n",
    "# Produces NFA as output.\n",
    "#\n",
    "# The NEW signifies that I'm generating NFAs starting from\n",
    "# sets of states.\n",
    "#\n",
    "# Adapted from calc.py that is available from \n",
    "# www.dabeaz.com/ply/example.html\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "#-- Begin lexer construction\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "#-- The tokens that constitute an RE are these\n",
    "tokens = (\n",
    "    'EPS','STR','LPAREN','RPAREN','PLUS','STAR'\n",
    "    )\n",
    "\n",
    "#-- The token definitions in terms of raw strings are being expressed now\n",
    "t_PLUS    = r'\\+'\n",
    "t_STAR    = r'\\*'\n",
    "t_LPAREN  = r'\\('\n",
    "t_RPAREN  = r'\\)'\n",
    "t_EPS     = r'\\'\\'|\\\"\\\"'  # Not allowing @ for empty string anymore!\n",
    "t_STR     = r'[a-zA-Z0-9]'  \n",
    "# Making the above r'[a-zA-Z0-9]+' to accept strings as \n",
    "# \"tokens\", i.e. indivisible units that can be subject to\n",
    "# RE operations\n",
    "\n",
    "#-- Ignored characters by the lexer\n",
    "t_ignore = \" \\t\"\n",
    "\n",
    "#-- Upon new lines, increase the lexer's line count variable\n",
    "def t_newline(t):\n",
    "    r'\\n+'\n",
    "    t.lexer.lineno += t.value.count(\"\\n\")\n",
    "\n",
    "#-- Lexer's error announcer for illegal characters\n",
    "def t_error(t):\n",
    "    print(\"Illegal character '%s'\" % t.value[0])\n",
    "    t.lexer.skip(1)\n",
    "    \n",
    "#-- We don't build lexer here; we build before calling yacc()\n",
    "#-- and then feed the lexer into reparser.parse (see below)\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "#--- Here is the parser set-up in terms of binary operator attributes\n",
    "#--------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "#-- Token precedences and associativity are declared in one place\n",
    "#-- By declaring PLUS before STAR, we are implying that it's of lower \n",
    "#-- precedence. Also declared is that they are both left-associative.\n",
    "\n",
    "precedence = (\n",
    "    ('left','PLUS'),\n",
    "    ('left','STAR'),\n",
    "    )\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "#--- Here are the parsing rules for REs; each returns an NFA as \"code\"\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "#-- * The E -> E + C production\n",
    "\n",
    "def p_expression_plus(t):\n",
    "    '''expression : expression PLUS catexp'''\n",
    "    t[0] = mk_plus_nfa(t[1], t[3]) # Union of the two NFAs is returned\n",
    "    \n",
    "def mk_plus_nfa(N1, N2):\n",
    "    \"\"\"Given two NFAs, return their union.\n",
    "    \"\"\"\n",
    "    delta_accum = dict({})\n",
    "    delta_accum.update(N1[\"Delta\"])\n",
    "    delta_accum.update(N2[\"Delta\"]) # Simply accumulate the transitions\n",
    "    # The alphabet is inferred bottom-up; thus we must union the Sigmas \n",
    "    # of the NFAs!\n",
    "    return mk_nfa(Q     = N1[\"Q\"] | N2[\"Q\"], \n",
    "                  Sigma = N1[\"Sigma\"] | N2[\"Sigma\"], \n",
    "                  Delta = delta_accum, \n",
    "                  Q0    = N1[\"Q0\"] | N2[\"Q0\"], \n",
    "                  F     = N1[\"F\"] | N2[\"F\"])    \n",
    "\n",
    "#-- * The E -> C production\n",
    "    \n",
    "def p_expression_plus_id(t):\n",
    "    '''expression : catexp'''\n",
    "    # Simply inherit the attribute from t[1] and pass on    \n",
    "    t[0] = t[1] \n",
    "\n",
    "#-- * The C -> C O production\n",
    "\n",
    "def p_expression_cat(t):\n",
    "    '''catexp :  catexp ordyexp'''\n",
    "    t[0] = mk_cat_nfa(t[1], t[2])\n",
    "\n",
    "def mk_cat_nfa(N1, N2):\n",
    "    delta_accum = dict({}) \n",
    "    delta_accum.update(N1[\"Delta\"])\n",
    "    delta_accum.update(N2[\"Delta\"])\n",
    "    # Now, introduce moves from every one of N1's final states\n",
    "    # to the set of N2's initial states.\n",
    "    for f in N1[\"F\"]:\n",
    "        # However, N1's final states may already have epsilon moves to\n",
    "        # other N1-states!\n",
    "        # Expand the target of such jumps to include N2's Q0 also!\n",
    "        if (f, \"\") in N1[\"Delta\"]: \n",
    "            delta_accum.update({ (f,\"\"):(N2[\"Q0\"] | N1[\"Delta\"][(f, \"\")])\n",
    "                               })\n",
    "        else:\n",
    "            delta_accum.update({ (f, \"\"): N2[\"Q0\"] })\n",
    "    # In syntax-directed translation, it is impossible\n",
    "    # that N2 and N1 have common states. Check anyhow\n",
    "    # in case there are bugs elsewhere that cause it.\n",
    "    assert((N2[\"F\"] & N1[\"F\"]) == set({})) \n",
    "    return mk_nfa(Q     = N1[\"Q\"] | N2[\"Q\"], \n",
    "                  Sigma = N1[\"Sigma\"] | N2[\"Sigma\"], \n",
    "                  Delta = delta_accum, \n",
    "                  Q0    = N1[\"Q0\"],\n",
    "                  F     = N2[\"F\"])\n",
    "\n",
    "#-- * The C -> O production\n",
    "\n",
    "def p_expression_cat_id(t):\n",
    "    '''catexp :  ordyexp'''\n",
    "    # Simply inherit the attribute from t[1] and pass on\n",
    "    t[0] = t[1]\n",
    "\n",
    "#-- * The O -> O STAR production\n",
    "\n",
    "def p_expression_ordy_star(t):\n",
    "    'ordyexp : ordyexp STAR'\n",
    "    t[0] = mk_star_nfa(t[1])\n",
    "\n",
    "def mk_star_nfa(N):\n",
    "    # Follow construction from Kozen's book:\n",
    "    # 1) Introduce new (single) start+final state IF\n",
    "    # 2) Let Q0 = set({ IF })\n",
    "    # 2) Move on epsilon from IF to the set N[Q0]\n",
    "    # 3) Make N[F] non-final\n",
    "    # 4) Spin back from every state in N[F] to Q0\n",
    "    #\n",
    "    delta_accum = dict({})\n",
    "    IF = NxtStateStr()\n",
    "    Q0 = set({ IF }) # new set of start + final states\n",
    "    # Jump from IF to N's start state set\n",
    "    delta_accum.update({ (IF,\"\"): N[\"Q0\"] })\n",
    "    delta_accum.update(N[\"Delta\"])\n",
    "    #\n",
    "    for f in N[\"F\"]:\n",
    "        # N's final states may already have epsilon moves to\n",
    "        # other N-states!\n",
    "        # Expand the target of such jumps to include Q0 also.\n",
    "        if (f, \"\") in N[\"Delta\"]:\n",
    "            delta_accum.update({ (f, \"\"): (Q0 | N[\"Delta\"][(f, \"\")]) })\n",
    "        else:\n",
    "            delta_accum.update({ (f, \"\"): Q0 })\n",
    "    #\n",
    "    return mk_nfa(Q     = N[\"Q\"] | Q0, \n",
    "                  Sigma = N[\"Sigma\"], \n",
    "                  Delta = delta_accum, \n",
    "                  Q0    = Q0, \n",
    "                  F     = Q0)\n",
    "\n",
    "#-- * The O -> ( E ) production\n",
    "\n",
    "def p_expression_ordy_paren(t):\n",
    "    'ordyexp : LPAREN expression RPAREN'\n",
    "    # Simply inherit the attribute from t[2] and pass on\n",
    "    t[0] = t[2]\n",
    "\n",
    "#-- * The O -> EPS production\n",
    "    \n",
    "def p_expression_ordy_eps(t):\n",
    "    'ordyexp : EPS'\n",
    "    t[0] = mk_eps_nfa()\n",
    "\n",
    "def mk_eps_nfa():\n",
    "    \"\"\"An nfa with exactly one start+final state\n",
    "    \"\"\"\n",
    "    Q0 = set({ NxtStateStr() })\n",
    "    F  = Q0\n",
    "    return mk_nfa(Q     = Q0, \n",
    "                  Sigma = set({}), \n",
    "                  Delta = dict({}), \n",
    "                  Q0    = Q0, \n",
    "                  F     = Q0)                      \n",
    "\n",
    "#-- * The O -> STR production, i.e. a single re letter\n",
    "\n",
    "def p_expression_ordy_str(t):\n",
    "    'ordyexp : STR'\n",
    "    t[0] = mk_symbol_nfa(t[1])\n",
    "\n",
    "def mk_symbol_nfa(a):\n",
    "    \"\"\"The NFA for a single re letter\n",
    "    \"\"\"\n",
    "    # Make a fresh initial state\n",
    "    q0 = NxtStateStr()\n",
    "    Q0 = set({ q0 })\n",
    "    # Make a fresh final state\n",
    "    f = NxtStateStr()\n",
    "    F = set({ f })\n",
    "    return mk_nfa(Q     = Q0 | F, \n",
    "                  Sigma = set({a}), \n",
    "                  Delta = { (q0,a): F },\n",
    "                  Q0    = Q0, \n",
    "                  F     = F)\n",
    "\n",
    "def p_error(t):\n",
    "    print(\"Syntax error at '%s'\" % t.value)\n",
    "\n",
    "#-- NOW BUILD THE PARSER if needed --    \n",
    "# parser = yacc()\n",
    "\n",
    "# End of reparseNEW.py\n",
    "# -----------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## RE to NFA code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def rename_nfa_states(N):\n",
    "    \"\"\"Tack-on I for initial, F for final, and IF for init+final states.\n",
    "       Return the renamed NFA.\n",
    "    \"\"\"\n",
    "    \n",
    "def re2nfa(s, stno = 0, IF=True):\n",
    "    \"\"\"Given a string s representing an RE and an optional\n",
    "       state number stno (default 0), generate an NFA that\n",
    "       is language equivalent to the RE. \n",
    "       If IF is set to false, the state renamings to follow\n",
    "       the convention of I, F, and IF are suppressed.\n",
    "    \"\"\"\n",
    "    # Reset the state number generator to 0\n",
    "    ResetStNum() \n",
    "    # NxtStateStr() gets called whenever needed. \n",
    "    # Defined in StateNameSanitizers.py\n",
    "\n",
    "    relexer = lex()\n",
    "\n",
    "    #-- NOW BUILD THE PARSER -- \n",
    "    reparser = yacc()\n",
    "    #-- FEED IT THE LEXER --\n",
    "    myparsednfa = reparser.parse(s, lexer=relexer)\n",
    "    #-- for debugging : return dotObj_nfa(myparsednfa, nfaname)\n",
    "    \n",
    "    if IF:\n",
    "        rename_nfa_states()\n",
    "        \n",
    "    return myparsednfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "print('''You may use any of these help commands:\n",
    "help(re2nfa)\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "318px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
